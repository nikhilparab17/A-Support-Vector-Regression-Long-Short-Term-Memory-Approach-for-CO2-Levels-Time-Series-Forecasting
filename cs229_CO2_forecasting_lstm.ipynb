{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cs229_CO2_forecasting_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1eIqJN8GTu6kXiWwwp7pmLHfo3DBbcdxJ",
      "authorship_tag": "ABX9TyPvVUiggMon2dIX5dkNJe/l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikhilparab17/ClimateChange-Analysis-and-Forecasting/blob/master/cs229_CO2_forecasting_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8bC13SKoaL_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "d467a81f-0da0-4066-cb42-1c5d6e42bac9"
      },
      "source": [
        "#include paths\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "import math\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from statsmodels.tools.eval_measures import rmse\n",
        "from keras.regularizers import l2\n",
        "\n",
        "\n",
        "# function to load data\n",
        "def loadExcelData(filepath):\n",
        "  print(\"Loading .xlsx data...\", filepath)\n",
        "  excelData = pd.ExcelFile(filepath)\n",
        "  data = excelData.parse()\n",
        "  return data\n",
        "\n",
        "# function to convert data to \n",
        "# pandas datetime series\n",
        "def convert_datetime(data):\n",
        "  data.index = data['YEAR'].apply(lambda x: dt.datetime.strptime(x, '%Y/%m/%d'))\n",
        "  del data['YEAR']\n",
        "  return data\n",
        "\n",
        "# function for data visualization\n",
        "def data_visualization(data, feature):\n",
        "  plt.plot(data.index, data)\n",
        "  plt.xlabel(\"Years\")\n",
        "  plt.ylabel(feature)\n",
        "  plt.title(feature + \" time-series\")\n",
        "  #plt.legend()\n",
        "  plt.show()\n",
        "  plt.clf()\n",
        "\n",
        "# function to transform (normalize) data \n",
        "def transform(train, test):\n",
        "  scalar = MinMaxScaler(feature_range=(0,1))\n",
        "  scalar.fit(train)\n",
        "\n",
        "  # transform train\n",
        "  train = train.reshape(train.shape[0], train.shape[1])\n",
        "  train_scaled = scalar.transform(train)\n",
        "\n",
        "  # transform test\n",
        "  test_scaled = test.reshape(test.shape[0], test.shape[1])\n",
        "  test_scaled = scalar.transform(test)\n",
        "\n",
        "  return scalar, train_scaled, test_scaled\n",
        "\n",
        "\n",
        "# function to build and fit LSTM model\n",
        "def fit_LSTM(x_train, y_train,\n",
        "             batch_size=1, num_epoch=100,\n",
        "             neurons=5, num_layers=1, \n",
        "             train_noise = True, reg_factor = 0.0001):\n",
        "  \n",
        "  model = Sequential()\n",
        "  # add lstm-layers\n",
        "  if num_layers > 1:\n",
        "    model.add(LSTM(neurons, input_shape=(x_train.shape[1],x_train.shape[2]), \n",
        "                   return_sequences=True,\n",
        "                   kernel_regularizer=l2(reg_factor), \n",
        "                   recurrent_regularizer=l2(reg_factor), \n",
        "                   bias_regularizer=l2(reg_factor)))\n",
        "    model.add(LSTM(neurons, return_sequences=False,\n",
        "                   kernel_regularizer=l2(reg_factor), \n",
        "                   recurrent_regularizer=l2(reg_factor), \n",
        "                   bias_regularizer=l2(reg_factor)))\n",
        "  else:\n",
        "    model.add(LSTM(neurons, input_shape=(x_train.shape[1], x_train.shape[2]), \n",
        "                   return_sequences=False,\n",
        "                   kernel_regularizer=l2(reg_factor), \n",
        "                   recurrent_regularizer=l2(reg_factor), \n",
        "                   bias_regularizer=l2(reg_factor)))\n",
        "  # dense layer\n",
        "  model.add(Dense(1))\n",
        "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "  train_error = []\n",
        "  if True:\n",
        "    x_train_orig = x_train\n",
        "\n",
        "    for it in np.arange(num_epoch):\n",
        "      if train_noise == True:\n",
        "        sample_noise = np.random.normal(0, 0.02,np.shape(x_train))\n",
        "        x_train = x_train_orig + sample_noise\n",
        "      else:\n",
        "        x_train = x_train_orig\n",
        "        \n",
        "      hist = model.fit(x_train, y_train, epochs=1,\n",
        "                     batch_size=batch_size, verbose=False, shuffle=False)\n",
        "      \n",
        "      train_error.append(hist.history['loss'])\n",
        "  else:\n",
        "    hist = model.fit(x_train, y_train, epochs=num_epoch,\n",
        "                     batch_size=batch_size, verbose=False, shuffle=False)\n",
        "    train_error = hist.history['loss']\n",
        "\n",
        "  \n",
        "  return [model, train_error]\n",
        "\n",
        "\n",
        "# function to convert input data into a lstm dataset format\n",
        "def create_dataset(dataset, look_back=1, index=0):\n",
        "\tdataX, dataY = [], []\n",
        "\tfor i in range(dataset.shape[0]-look_back-1):\n",
        "\t\ta = dataset[i:(i+look_back), index]\n",
        "\t\tdataX.append(a)\n",
        "\t\tdataY.append(dataset[i + look_back, index])\n",
        "\treturn np.array(dataX), np.array(dataY)\n",
        "\n",
        "# function to train model and predict output\n",
        "def train_and_predict(data, hypothesis = 1,\n",
        "                      look_back = 2, num_features = 2, add_noise = True, \n",
        "                      num_layers = 1, num_neurons = 4, num_epochs = 100,\n",
        "                      batch_size = 5, regularization_factor = 0.0001):\n",
        "\n",
        "  # convert time to numpy array as required for training\n",
        "  data_numpy = data.to_numpy()\n",
        "\n",
        "  #split data into train and val set\n",
        "  split = data_numpy.shape[0] - int((0.2*data_numpy.shape[0]))\n",
        "  train, val = data_numpy[:split,:],data_numpy[split:,:]\n",
        "  \n",
        "  # transform only CO2 data for recovering CO2 from 'scalar_co2'\n",
        "  train_co2, val_co2 = data_numpy[:split,0], data_numpy[split:,0] \n",
        "  scaler_co2, train_co2_scaled, val_co2_scaled = transform(train_co2.reshape(-1,1),\n",
        "                                                           val_co2.reshape(-1,1))\n",
        "\n",
        "  # transform whole data\n",
        "  scaler, train_scaled, val_scaled = transform(train, val)\n",
        "\n",
        "  # create lookback dataset\n",
        "  if hypothesis == 2:\n",
        "    # hyothesis-2 (CO2 levels + GDP)\n",
        "    \n",
        "    # feature-CO2\n",
        "    trainX_CO2, trainY_CO2 = create_dataset(train_scaled, look_back, 0)\n",
        "    valX_CO2, valY_CO2 = create_dataset(val_scaled, look_back, 0)\n",
        "    trainX_CO2 = np.reshape(trainX_CO2, (trainX_CO2.shape[0],1, trainX_CO2.shape[1]))\n",
        "    valX_CO2 = np.reshape(valX_CO2, (valX_CO2.shape[0],1, valX_CO2.shape[1]))\n",
        "    \n",
        "    # feature-GDP\n",
        "    trainX_GDP, trainY_GDP = create_dataset(train_scaled, look_back, 1)\n",
        "    valX_GDP, valY_GDP = create_dataset(val_scaled, look_back, 1)\n",
        "    trainX_GDP = np.reshape(trainX_GDP ,(trainX_GDP.shape[0],1, trainX_GDP.shape[1]))\n",
        "    valX_GDP = np.reshape(valX_GDP ,(valX_GDP.shape[0],1, valX_GDP.shape[1]))\n",
        "    \n",
        "    #concatenate features\n",
        "    trainX = np.concatenate((trainX_CO2, trainX_GDP), axis = 1)\n",
        "    valX = np.concatenate((valX_CO2, valX_GDP), axis = 1)\n",
        "\n",
        "  elif hypothesis == 3:\n",
        "    # hyothesis-3 (CO2 levels + Crude Oil)\n",
        "\n",
        "    # feature-CO2\n",
        "    trainX_CO2, trainY_CO2 = create_dataset(train_scaled, look_back, 0)\n",
        "    valX_CO2, valY_CO2 = create_dataset(val_scaled, look_back, 0)\n",
        "    trainX_CO2 = np.reshape(trainX_CO2, (trainX_CO2.shape[0],1, trainX_CO2.shape[1]))\n",
        "    valX_CO2 = np.reshape(valX_CO2, (valX_CO2.shape[0],1, valX_CO2.shape[1]))\n",
        "    \n",
        "\n",
        "    # feature-OIL\n",
        "    trainX_OIL, trainY_OIL = create_dataset(train_scaled, look_back, 2)\n",
        "    valX_OIL, valY_OIL = create_dataset(val_scaled, look_back, 2)\n",
        "    trainX_OIL = np.reshape(trainX_OIL ,(trainX_OIL.shape[0],1, trainX_OIL.shape[1]))\n",
        "    valX_OIL = np.reshape(valX_OIL ,(valX_OIL.shape[0],1, valX_OIL.shape[1]))\n",
        "\n",
        "    #concatenate features\n",
        "    trainX = np.concatenate((trainX_CO2, trainX_OIL), axis = 1)\n",
        "    valX = np.concatenate((valX_CO2, valX_OIL), axis = 1)\n",
        "\n",
        "  elif hypothesis == 4:\n",
        "    # hyothesis-4 (CO2 levels + GDP + Crude OIL)\n",
        "\n",
        "    # feature-CO2\n",
        "    trainX_CO2, trainY_CO2 = create_dataset(train_scaled, look_back, 0)\n",
        "    valX_CO2, valY_CO2 = create_dataset(val_scaled, look_back, 0)\n",
        "    trainX_CO2 = np.reshape(trainX_CO2, (trainX_CO2.shape[0],1, trainX_CO2.shape[1]))\n",
        "    valX_CO2 = np.reshape(valX_CO2, (valX_CO2.shape[0],1, valX_CO2.shape[1]))\n",
        "    \n",
        "    # feature-GDP\n",
        "    trainX_GDP, trainY_GDP = create_dataset(train_scaled, look_back, 1)\n",
        "    valX_GDP, valY_GDP = create_dataset(val_scaled, look_back, 1)\n",
        "    trainX_GDP = np.reshape(trainX_GDP ,(trainX_GDP.shape[0],1, trainX_GDP.shape[1]))\n",
        "    valX_GDP = np.reshape(valX_GDP ,(valX_GDP.shape[0],1, valX_GDP.shape[1]))\n",
        "\n",
        "    # feature-OIL\n",
        "    trainX_OIL, trainY_OIL = create_dataset(train_scaled, look_back, 2)\n",
        "    valX_OIL, valY_OIL = create_dataset(val_scaled, look_back, 2)\n",
        "    trainX_OIL = np.reshape(trainX_OIL ,(trainX_OIL.shape[0],1, trainX_OIL.shape[1]))\n",
        "    valX_OIL = np.reshape(valX_OIL ,(valX_OIL.shape[0],1, valX_OIL.shape[1]))\n",
        "    \n",
        "    #concatenate features\n",
        "    trainX = np.concatenate((trainX_CO2, trainX_GDP, trainX_OIL), axis = 1)\n",
        "    valX = np.concatenate((valX_CO2, valX_GDP, valX_OIL), axis = 1)\n",
        "\n",
        "  else: \n",
        "    # hypothesis-1 (CO2 levels)\n",
        "    # feature-CO2\n",
        "    trainX_CO2, trainY_CO2 = create_dataset(train_scaled, look_back, 0)\n",
        "    valX_CO2, valY_CO2 = create_dataset(val_scaled, look_back, 0)\n",
        "    trainX_CO2 = np.reshape(trainX_CO2, (trainX_CO2.shape[0],1, trainX_CO2.shape[1]))\n",
        "    valX_CO2 = np.reshape(valX_CO2, (valX_CO2.shape[0],1, valX_CO2.shape[1]))\n",
        "    trainX = trainX_CO2\n",
        "    valX = valX_CO2\n",
        "\n",
        "\n",
        "  trainY = trainY_CO2\n",
        "  valY  = valY_CO2\n",
        "  #fit model\n",
        "  print(\"create and fit LSTM model..\")\n",
        "  [model, epoch_error] = fit_LSTM(trainX, trainY, \n",
        "                                  batch_size=batch_size, num_epoch=num_epochs, \n",
        "                                  neurons = num_neurons, num_layers= num_layers, \n",
        "                                  train_noise = add_noise, \n",
        "                                  reg_factor = regularization_factor)\n",
        "\n",
        "\n",
        "  # forecast the training data to build up state for forecasting\n",
        "  trainPredict = model.predict(trainX)\n",
        "  valPredict = model.predict(valX)\n",
        "\n",
        "  # invert transformation\n",
        "  trainPredict = scaler_co2.inverse_transform(trainPredict)\n",
        "  trainY = scaler_co2.inverse_transform([trainY])\n",
        "  valPredict = scaler_co2.inverse_transform(valPredict)\n",
        "  valY = scaler_co2.inverse_transform([valY])\n",
        "    \n",
        "  # calculate root mean squared error\n",
        "  print(\"compute rms error... \")\n",
        "  trainScore = rmse(trainY[0,:], trainPredict[:,0])\n",
        "  valScore = rmse(valY[0,:], valPredict[:,0])\n",
        "    \n",
        "  # shift train predictions for plotting\n",
        "  print(\"plot and save results... \")\n",
        "  trainPredictPlot = np.empty_like(data_numpy)\n",
        "  trainPredictPlot[:, :] = np.nan\n",
        "  trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
        "\n",
        "  # shift test predictions for plotting\n",
        "  valPredictPlot = np.empty_like(data_numpy)\n",
        "  valPredictPlot[:, :] = np.nan\n",
        "  valPredictPlot[len(trainPredict)+ (look_back*2) + 1:len(data_numpy)-1, :] = valPredict\n",
        "\n",
        "  # plot baseline, predictions and save results\n",
        "  #if hypothesis == 1:\n",
        "  plt.plot(data.index, data[\"CO2 Levels\"], label = \"CO2 Levels (ppm)\")\n",
        "  plt.plot(data.index, valPredictPlot[:,0], \n",
        "           label = \"Predictions Model \" + str(hypothesis))\n",
        "  plt.xlabel(\"Years\")\n",
        "  plt.ylabel(\"CO2 Levels (ppm)\")\n",
        "  plt.savefig(\"/content/drive/My Drive/Colab Notebooks/cs229/lstm_prediction.png\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  plt.clf()\n",
        "\n",
        "  # plot train error\n",
        "  plt.plot(epoch_error, \n",
        "           label = \"Predictions Model \" + str(hypothesis))\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Training Loss (RMSE)\")\n",
        "  plt.legend()\n",
        "  plt.savefig(\"/content/drive/My Drive/Colab Notebooks/cs229/lstm_trainingloss.png\")\n",
        "  plt.show()\n",
        "  plt.clf()\n",
        "\n",
        "  # plot model\n",
        "  print(model.summary())\n",
        "  plot_model(model, \n",
        "             to_file='/content/drive/My Drive/Colab Notebooks/cs229/model_' + str(hypothesis) + '.png', \n",
        "             show_shapes=True, show_layer_names=False)\n",
        "  print('RMSE: Train Score: %.3f Test Score: %.3f RMSE' % (trainScore, valScore))\n",
        "\n",
        "  return valScore\n",
        "\n",
        "\n",
        "# k-fold nested cross validation for time series data\n",
        "def run_cross_validation(model_hyp = 1, \n",
        "                         model_features = 1, model_look_back_length = 1,\n",
        "                         model_num_layers = 1, model_num_neurons_per_layer = 5,\n",
        "                         model_regularization_factor = 0.001, train_add_noise = True, \n",
        "                         train_num_epochs = 100, train_batch_size = 5):\n",
        "  cross_val = []\n",
        "  cross_val_rms = []\n",
        "  kval_nested_kfold = 7\n",
        "  csv_file_path = \"/content/drive/My Drive/Colab Notebooks/cs229/CV/co2_monthly_process_up_\"\n",
        "  csv_file_ext = \".xlsx\"\n",
        "\n",
        "  for k in np.arange(kval_nested_kfold):\n",
        "    s = csv_file_path + str(k+1) + csv_file_ext\n",
        "    csv_data = loadExcelData(s)\n",
        "    cross_val.append(convert_datetime(csv_data))\n",
        "\n",
        "  for i in np.arange(len(cross_val)):\n",
        "    # run lstm-model\n",
        "    error = train_and_predict(cross_val[i],hypothesis = model_hyp , \n",
        "                              look_back = model_look_back_length,\n",
        "                              num_features = model_features,\n",
        "                              num_layers = model_num_layers,\n",
        "                              num_neurons = model_num_neurons_per_layer,\n",
        "                              add_noise = train_add_noise,\n",
        "                              regularization_factor = model_regularization_factor,\n",
        "                              num_epochs = train_num_epochs, batch_size = train_batch_size)\n",
        "    cross_val_rms.append(error)\n",
        "    \n",
        "\n",
        "  print(\"Cross-Val RMS Error:\", np.around(cross_val_rms, 3))\n",
        "  return np.mean(cross_val_rms)\n",
        "\n",
        "\n",
        "def main(data_visualization = False,\n",
        "         model_hypothesis = 1,\n",
        "         model_train_and_cross_validation = True,\n",
        "         model_test_evaluation = True):\n",
        "  \n",
        "    print('Start Program...')\n",
        "\n",
        "    df = loadExcelData(\"\\data\\co2_dataset_preprocess.xlsx\")\n",
        "\n",
        "    # convert to datetime\n",
        "    df = convert_datetime(df)\n",
        "\n",
        "    #VISUALIZATION= data_visualization\n",
        "    #CROSS_VAL_MODEL_SELECTION = True\n",
        "    #FINAL_MODEL_EVAL = True\n",
        "    \n",
        "    if data_visualization == True:\n",
        "      # data visualization\n",
        "      data_visualization(df[\"CO2 Levels\"], \"CO2 Levels (ppm)\")\n",
        "      data_visualization(df[\"GDP\"], \"GDP (millions USD)\")\n",
        "      data_visualization(df[\"Crude Oil\"], \"Crude Oil (USD)\")\n",
        "\n",
        "    if model_train_and_cross_validation == True:\n",
        "      \n",
        "        print(\"\\nRunning Cross-Validation for Hypothesis \" + str(model_hypothesis) + \"...\")\n",
        "        if model_hypothesis == 2 or model_hypothesis == 3:\n",
        "          hyp_features = 2\n",
        "        if model_hypothesis == 4:\n",
        "          hyp_features = 3\n",
        "        if model_hypothesis == 1:\n",
        "          hyp_features = 1\n",
        "\n",
        "        ############# HYPERPARAMETER TUNING PARAMETERS #########################\n",
        "        hyp_look_back = 12\n",
        "        hyp_regularization_factor = 0.0001\n",
        "        hyp_layers = 2\n",
        "        hyp_neurons = 10\n",
        "        #######################################################################\n",
        "        \n",
        "        hyp_add_train_noise = True\n",
        "        avg_error = run_cross_validation(model_hyp = model_hypothesis, \n",
        "                                         model_features = hyp_features,\n",
        "                                         model_look_back_length = hyp_look_back,\n",
        "                                         model_num_layers = hyp_layers,\n",
        "                                         model_num_neurons_per_layer = hyp_neurons,\n",
        "                                         model_regularization_factor = hyp_regularization_factor,\n",
        "                                         train_add_noise = hyp_add_train_noise\n",
        "                                     )\n",
        "        print(\"Hypothesis: \" + str(CROSS_VAL_HYPOTHESIS) + \", CROSS-VAL AVG. RMSE ERROR:\", avg_error)\n",
        "    \n",
        "    \n",
        "    if model_test_evaluation == True:\n",
        "      \n",
        "      ############ test evaluation on winner hypothesis-1 ######################\n",
        "      print(\"\\nwinner hypothesis-1 (CO2 levels) evaluation on test dataset\")\n",
        "      model_hypothesis = 1\n",
        "      hyp_features = 1\n",
        "      hyp_look_back = 1\n",
        "      hyp_regularization_factor = 0.0001\n",
        "      hyp_layers = 1\n",
        "      hyp_neurons = 5\n",
        "      hyp_add_train_noise = True\n",
        "\n",
        "      # run lstm-model\n",
        "      error = train_and_predict(df, hypothesis = model_hypothesis,\n",
        "                                look_back = hyp_look_back,\n",
        "                                num_features = hyp_features, \n",
        "                                num_layers = hyp_layers,\n",
        "                                num_neurons = hyp_neurons,\n",
        "                                add_noise = hyp_add_train_noise,\n",
        "                                regularization_factor = hyp_regularization_factor)\n",
        "      ##########################################################################\n",
        "\n",
        "      \n",
        "      ############ test evaluation on winner hypothesis-2 ######################\n",
        "      print(\"\\nwinner hypothesis-2 (CO2 levels + GDP) evaluation on test dataset\")\n",
        "      model_hypothesis = 2\n",
        "      hyp_features = 2\n",
        "      hyp_look_back = 2\n",
        "      hyp_regularization_factor = 0.0001\n",
        "      hyp_layers = 2\n",
        "      hyp_neurons = 10\n",
        "      hyp_add_train_noise = True\n",
        "\n",
        "      # run lstm-model\n",
        "      error = train_and_predict(df, hypothesis = model_hypothesis,\n",
        "                                look_back = hyp_look_back,\n",
        "                                num_features = hyp_features, num_layers = hyp_layers,\n",
        "                                num_neurons = hyp_neurons,\n",
        "                                add_noise = hyp_add_train_noise,\n",
        "                                regularization_factor = hyp_regularization_factor)\n",
        "      ##########################################################################\n",
        "\n",
        "      ############ test evaluation on winner hypothesis-3 ######################\n",
        "      print(\"\\nwinner hypothesis-3 (CO2 levels + Crude Oil) evaluation on test dataset\")\n",
        "      model_hypothesis = 3\n",
        "      hyp_features = 2\n",
        "      hyp_look_back = 2\n",
        "      hyp_regularization_factor = 0.0001\n",
        "      hyp_layers = 2\n",
        "      hyp_neurons = 10\n",
        "      hyp_add_train_noise = True\n",
        "\n",
        "      # run lstm-model\n",
        "      error = train_and_predict( df, hypothesis = model_hypothesis,\n",
        "                                look_back = hyp_look_back,\n",
        "                                num_features = hyp_features,\n",
        "                                num_layers = hyp_layers,\n",
        "                                num_neurons = hyp_neurons,\n",
        "                                add_noise = hyp_add_train_noise,\n",
        "                                regularization_factor = hyp_regularization_factor)\n",
        "      ##########################################################################\n",
        "      \n",
        "      ############ test evaluation on winner hypothesis-4 ######################\n",
        "      print(\"\\nwinner hypothesis-4 (CO2 levels + GDP + Crude Oil) evaluation on test dataset\")\n",
        "      model_hypothesis = 4\n",
        "      hyp_features = 3\n",
        "      hyp_look_back = 2\n",
        "      hyp_regularization_factor = 0.0001\n",
        "      hyp_layers = 2\n",
        "      hyp_neurons = 10\n",
        "      hyp_add_train_noise = True\n",
        "\n",
        "      # run lstm-model\n",
        "      error = train_and_predict(df, hypothesis = model_hypothesis,\n",
        "                                look_back = hyp_look_back,\n",
        "                                num_features = hyp_features, num_layers = hyp_layers,\n",
        "                                num_neurons = hyp_neurons,\n",
        "                                add_noise = hyp_add_train_noise,\n",
        "                                regularization_factor = hyp_regularization_factor)\n",
        "      ##########################################################################\n",
        "      \n",
        "      #plt.savefig(\"/content/drive/My Drive/Colab Notebooks/cs229/lstm_prediction_new.png\")\n",
        "      #plt.show()\n",
        "\n",
        "    print('End of Program...')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main(data_visualization = True,\n",
        "       model_hypothesis = 1,\n",
        "       model_train_and_cross_validation = False,\n",
        "       model_test_evaluation = False)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Program...\n",
            "Loading .xlsx data... \\data\\co2_dataset_preprocess.xlsx\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-4b40737ed732>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    457\u001b[0m        \u001b[0mmodel_hypothesis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m        \u001b[0mmodel_train_and_cross_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m        model_test_evaluation = False)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-4b40737ed732>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(data_visualization, model_hypothesis, model_train_and_cross_validation, model_test_evaluation)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Start Program...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadExcelData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\data\\co2_dataset_preprocess.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;31m# convert to datetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-4b40737ed732>\u001b[0m in \u001b[0;36mloadExcelData\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloadExcelData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading .xlsx data...\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   \u001b[0mexcelData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexcelData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine)\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstringify_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Install xlrd >= 1.0.0 for Excel support\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xlrd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36mload_workbook\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_contents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '\\\\data\\\\co2_dataset_preprocess.xlsx'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIWbv7mDlipQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}